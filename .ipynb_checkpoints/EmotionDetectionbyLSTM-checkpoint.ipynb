{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fcfbbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e7171af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># mean_0_a</th>\n",
       "      <th>mean_1_a</th>\n",
       "      <th>mean_2_a</th>\n",
       "      <th>mean_3_a</th>\n",
       "      <th>mean_4_a</th>\n",
       "      <th>mean_d_0_a</th>\n",
       "      <th>mean_d_1_a</th>\n",
       "      <th>mean_d_2_a</th>\n",
       "      <th>mean_d_3_a</th>\n",
       "      <th>mean_d_4_a</th>\n",
       "      <th>...</th>\n",
       "      <th>fft_741_b</th>\n",
       "      <th>fft_742_b</th>\n",
       "      <th>fft_743_b</th>\n",
       "      <th>fft_744_b</th>\n",
       "      <th>fft_745_b</th>\n",
       "      <th>fft_746_b</th>\n",
       "      <th>fft_747_b</th>\n",
       "      <th>fft_748_b</th>\n",
       "      <th>fft_749_b</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.62</td>\n",
       "      <td>30.3</td>\n",
       "      <td>-356.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>26.3</td>\n",
       "      <td>1.070</td>\n",
       "      <td>0.411</td>\n",
       "      <td>-15.70</td>\n",
       "      <td>2.06</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>23.5</td>\n",
       "      <td>20.3</td>\n",
       "      <td>20.3</td>\n",
       "      <td>23.5</td>\n",
       "      <td>-215.0</td>\n",
       "      <td>280.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>280.00</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.80</td>\n",
       "      <td>33.1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>25.8</td>\n",
       "      <td>22.8</td>\n",
       "      <td>6.550</td>\n",
       "      <td>1.680</td>\n",
       "      <td>2.88</td>\n",
       "      <td>3.83</td>\n",
       "      <td>-4.82</td>\n",
       "      <td>...</td>\n",
       "      <td>-23.3</td>\n",
       "      <td>-21.8</td>\n",
       "      <td>-21.8</td>\n",
       "      <td>-23.3</td>\n",
       "      <td>182.0</td>\n",
       "      <td>2.57</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>2.57</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.90</td>\n",
       "      <td>29.4</td>\n",
       "      <td>-416.0</td>\n",
       "      <td>16.7</td>\n",
       "      <td>23.7</td>\n",
       "      <td>79.900</td>\n",
       "      <td>3.360</td>\n",
       "      <td>90.20</td>\n",
       "      <td>89.90</td>\n",
       "      <td>2.03</td>\n",
       "      <td>...</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-267.0</td>\n",
       "      <td>281.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>281.00</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.90</td>\n",
       "      <td>31.6</td>\n",
       "      <td>-143.0</td>\n",
       "      <td>19.8</td>\n",
       "      <td>24.3</td>\n",
       "      <td>-0.584</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>8.82</td>\n",
       "      <td>2.30</td>\n",
       "      <td>-1.97</td>\n",
       "      <td>...</td>\n",
       "      <td>299.0</td>\n",
       "      <td>-243.0</td>\n",
       "      <td>-243.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>9.53</td>\n",
       "      <td>9.53</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.30</td>\n",
       "      <td>31.3</td>\n",
       "      <td>45.2</td>\n",
       "      <td>27.3</td>\n",
       "      <td>24.5</td>\n",
       "      <td>34.800</td>\n",
       "      <td>-5.790</td>\n",
       "      <td>3.06</td>\n",
       "      <td>41.40</td>\n",
       "      <td>5.52</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>38.1</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.90</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2549 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   # mean_0_a  mean_1_a  mean_2_a  mean_3_a  mean_4_a  mean_d_0_a  mean_d_1_a  \\\n",
       "0        4.62      30.3    -356.0      15.6      26.3       1.070       0.411   \n",
       "1       28.80      33.1      32.0      25.8      22.8       6.550       1.680   \n",
       "2        8.90      29.4    -416.0      16.7      23.7      79.900       3.360   \n",
       "3       14.90      31.6    -143.0      19.8      24.3      -0.584      -0.284   \n",
       "4       28.30      31.3      45.2      27.3      24.5      34.800      -5.790   \n",
       "\n",
       "   mean_d_2_a  mean_d_3_a  mean_d_4_a  ...  fft_741_b  fft_742_b  fft_743_b  \\\n",
       "0      -15.70        2.06        3.15  ...       23.5       20.3       20.3   \n",
       "1        2.88        3.83       -4.82  ...      -23.3      -21.8      -21.8   \n",
       "2       90.20       89.90        2.03  ...      462.0     -233.0     -233.0   \n",
       "3        8.82        2.30       -1.97  ...      299.0     -243.0     -243.0   \n",
       "4        3.06       41.40        5.52  ...       12.0       38.1       38.1   \n",
       "\n",
       "   fft_744_b  fft_745_b  fft_746_b  fft_747_b  fft_748_b  fft_749_b     label  \n",
       "0       23.5     -215.0     280.00    -162.00    -162.00     280.00  NEGATIVE  \n",
       "1      -23.3      182.0       2.57     -31.60     -31.60       2.57   NEUTRAL  \n",
       "2      462.0     -267.0     281.00    -148.00    -148.00     281.00  POSITIVE  \n",
       "3      299.0      132.0     -12.40       9.53       9.53     -12.40  POSITIVE  \n",
       "4       12.0      119.0     -17.60      23.90      23.90     -17.60   NEUTRAL  \n",
       "\n",
       "[5 rows x 2549 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('H:/emotions.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca91a4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2132 entries, 0 to 2131\n",
      "Columns: 2549 entries, # mean_0_a to label\n",
      "dtypes: float64(2548), object(1)\n",
      "memory usage: 41.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6460325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='label', ylabel='count'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(x='label', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02dabb98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# mean_0_a    0\n",
       "mean_1_a      0\n",
       "mean_2_a      0\n",
       "mean_3_a      0\n",
       "mean_4_a      0\n",
       "             ..\n",
       "fft_746_b     0\n",
       "fft_747_b     0\n",
       "fft_748_b     0\n",
       "fft_749_b     0\n",
       "label         0\n",
       "Length: 2549, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()\n",
    "# no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7618375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   # mean_0_a  mean_1_a  mean_2_a  mean_3_a  mean_4_a  mean_d_0_a  mean_d_1_a  \\\n",
      "0        4.62      30.3    -356.0      15.6      26.3       1.070       0.411   \n",
      "1       28.80      33.1      32.0      25.8      22.8       6.550       1.680   \n",
      "2        8.90      29.4    -416.0      16.7      23.7      79.900       3.360   \n",
      "3       14.90      31.6    -143.0      19.8      24.3      -0.584      -0.284   \n",
      "4       28.30      31.3      45.2      27.3      24.5      34.800      -5.790   \n",
      "\n",
      "   mean_d_2_a  mean_d_3_a  mean_d_4_a  ...  fft_741_b  fft_742_b  fft_743_b  \\\n",
      "0      -15.70        2.06        3.15  ...       23.5       20.3       20.3   \n",
      "1        2.88        3.83       -4.82  ...      -23.3      -21.8      -21.8   \n",
      "2       90.20       89.90        2.03  ...      462.0     -233.0     -233.0   \n",
      "3        8.82        2.30       -1.97  ...      299.0     -243.0     -243.0   \n",
      "4        3.06       41.40        5.52  ...       12.0       38.1       38.1   \n",
      "\n",
      "   fft_744_b  fft_745_b  fft_746_b  fft_747_b  fft_748_b  fft_749_b  label  \n",
      "0       23.5     -215.0     280.00    -162.00    -162.00     280.00      2  \n",
      "1      -23.3      182.0       2.57     -31.60     -31.60       2.57      0  \n",
      "2      462.0     -267.0     281.00    -148.00    -148.00     281.00      1  \n",
      "3      299.0      132.0     -12.40       9.53       9.53     -12.40      1  \n",
      "4       12.0      119.0     -17.60      23.90      23.90     -17.60      0  \n",
      "\n",
      "[5 rows x 2549 columns]\n",
      "0    716\n",
      "2    708\n",
      "1    708\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "encode = ({'NEUTRAL': 0, 'POSITIVE': 1, 'NEGATIVE': 2} )\n",
    "#new dataset with replaced values\n",
    "df_encoded = df.replace(encode)\n",
    "\n",
    "print(df_encoded.head())\n",
    "print(df_encoded['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd8cad1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_encoded['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3b1c9e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># mean_0_a</th>\n",
       "      <th>mean_1_a</th>\n",
       "      <th>mean_2_a</th>\n",
       "      <th>mean_3_a</th>\n",
       "      <th>mean_4_a</th>\n",
       "      <th>mean_d_0_a</th>\n",
       "      <th>mean_d_1_a</th>\n",
       "      <th>mean_d_2_a</th>\n",
       "      <th>mean_d_3_a</th>\n",
       "      <th>mean_d_4_a</th>\n",
       "      <th>...</th>\n",
       "      <th>fft_741_b</th>\n",
       "      <th>fft_742_b</th>\n",
       "      <th>fft_743_b</th>\n",
       "      <th>fft_744_b</th>\n",
       "      <th>fft_745_b</th>\n",
       "      <th>fft_746_b</th>\n",
       "      <th>fft_747_b</th>\n",
       "      <th>fft_748_b</th>\n",
       "      <th>fft_749_b</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.62</td>\n",
       "      <td>30.3</td>\n",
       "      <td>-356.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>26.3</td>\n",
       "      <td>1.070</td>\n",
       "      <td>0.411</td>\n",
       "      <td>-15.70</td>\n",
       "      <td>2.06</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>23.5</td>\n",
       "      <td>20.3</td>\n",
       "      <td>20.3</td>\n",
       "      <td>23.5</td>\n",
       "      <td>-215.0</td>\n",
       "      <td>280.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>280.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.80</td>\n",
       "      <td>33.1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>25.8</td>\n",
       "      <td>22.8</td>\n",
       "      <td>6.550</td>\n",
       "      <td>1.680</td>\n",
       "      <td>2.88</td>\n",
       "      <td>3.83</td>\n",
       "      <td>-4.82</td>\n",
       "      <td>...</td>\n",
       "      <td>-23.3</td>\n",
       "      <td>-21.8</td>\n",
       "      <td>-21.8</td>\n",
       "      <td>-23.3</td>\n",
       "      <td>182.0</td>\n",
       "      <td>2.57</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>2.57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.90</td>\n",
       "      <td>29.4</td>\n",
       "      <td>-416.0</td>\n",
       "      <td>16.7</td>\n",
       "      <td>23.7</td>\n",
       "      <td>79.900</td>\n",
       "      <td>3.360</td>\n",
       "      <td>90.20</td>\n",
       "      <td>89.90</td>\n",
       "      <td>2.03</td>\n",
       "      <td>...</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-267.0</td>\n",
       "      <td>281.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>281.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.90</td>\n",
       "      <td>31.6</td>\n",
       "      <td>-143.0</td>\n",
       "      <td>19.8</td>\n",
       "      <td>24.3</td>\n",
       "      <td>-0.584</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>8.82</td>\n",
       "      <td>2.30</td>\n",
       "      <td>-1.97</td>\n",
       "      <td>...</td>\n",
       "      <td>299.0</td>\n",
       "      <td>-243.0</td>\n",
       "      <td>-243.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>9.53</td>\n",
       "      <td>9.53</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.30</td>\n",
       "      <td>31.3</td>\n",
       "      <td>45.2</td>\n",
       "      <td>27.3</td>\n",
       "      <td>24.5</td>\n",
       "      <td>34.800</td>\n",
       "      <td>-5.790</td>\n",
       "      <td>3.06</td>\n",
       "      <td>41.40</td>\n",
       "      <td>5.52</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>38.1</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.90</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2549 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   # mean_0_a  mean_1_a  mean_2_a  mean_3_a  mean_4_a  mean_d_0_a  mean_d_1_a  \\\n",
       "0        4.62      30.3    -356.0      15.6      26.3       1.070       0.411   \n",
       "1       28.80      33.1      32.0      25.8      22.8       6.550       1.680   \n",
       "2        8.90      29.4    -416.0      16.7      23.7      79.900       3.360   \n",
       "3       14.90      31.6    -143.0      19.8      24.3      -0.584      -0.284   \n",
       "4       28.30      31.3      45.2      27.3      24.5      34.800      -5.790   \n",
       "\n",
       "   mean_d_2_a  mean_d_3_a  mean_d_4_a  ...  fft_741_b  fft_742_b  fft_743_b  \\\n",
       "0      -15.70        2.06        3.15  ...       23.5       20.3       20.3   \n",
       "1        2.88        3.83       -4.82  ...      -23.3      -21.8      -21.8   \n",
       "2       90.20       89.90        2.03  ...      462.0     -233.0     -233.0   \n",
       "3        8.82        2.30       -1.97  ...      299.0     -243.0     -243.0   \n",
       "4        3.06       41.40        5.52  ...       12.0       38.1       38.1   \n",
       "\n",
       "   fft_744_b  fft_745_b  fft_746_b  fft_747_b  fft_748_b  fft_749_b  label  \n",
       "0       23.5     -215.0     280.00    -162.00    -162.00     280.00      2  \n",
       "1      -23.3      182.0       2.57     -31.60     -31.60       2.57      0  \n",
       "2      462.0     -267.0     281.00    -148.00    -148.00     281.00      1  \n",
       "3      299.0      132.0     -12.40       9.53       9.53     -12.40      1  \n",
       "4       12.0      119.0     -17.60      23.90      23.90     -17.60      0  \n",
       "\n",
       "[5 rows x 2549 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc51b6fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2132, 2548)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=df_encoded.drop([\"label\"]  ,axis=1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00d4d4c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2132,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df_encoded.loc[:,'label'].values\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce734b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x)\n",
    "x = scaler.transform(x)\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y = to_categorical(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b5e64aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e52f282",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "063e2c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.reshape(x_train, (x_train.shape[0],1,x.shape[1]))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0],1,x.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76a3eaed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 1, 64)             668928    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1, 64)             0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 32)                12416     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 681,443\n",
      "Trainable params: 681,443\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(1,2548),activation=\"relu\",return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(32,activation=\"sigmoid\"))\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(LSTM(100,return_sequences=True))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(LSTM(50))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = \"adam\", metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f91a52f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "54/54 [==============================] - 2s 12ms/step - loss: 0.5956 - accuracy: 0.7977 - val_loss: 0.3654 - val_accuracy: 0.9180\n",
      "Epoch 2/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.3100 - accuracy: 0.9337 - val_loss: 0.2803 - val_accuracy: 0.9180\n",
      "Epoch 3/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.2199 - accuracy: 0.9431 - val_loss: 0.2191 - val_accuracy: 0.9344\n",
      "Epoch 4/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.1894 - accuracy: 0.9443 - val_loss: 0.1975 - val_accuracy: 0.9321\n",
      "Epoch 5/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.1487 - accuracy: 0.9630 - val_loss: 0.1980 - val_accuracy: 0.9227\n",
      "Epoch 6/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.1246 - accuracy: 0.9683 - val_loss: 0.1661 - val_accuracy: 0.9368\n",
      "Epoch 7/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.1064 - accuracy: 0.9730 - val_loss: 0.1513 - val_accuracy: 0.9391\n",
      "Epoch 8/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0749 - accuracy: 0.9848 - val_loss: 0.1171 - val_accuracy: 0.9672\n",
      "Epoch 9/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0545 - accuracy: 0.9889 - val_loss: 0.1206 - val_accuracy: 0.9672\n",
      "Epoch 10/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0454 - accuracy: 0.9935 - val_loss: 0.1480 - val_accuracy: 0.9602\n",
      "Epoch 11/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0403 - accuracy: 0.9941 - val_loss: 0.1191 - val_accuracy: 0.9696\n",
      "Epoch 12/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0365 - accuracy: 0.9947 - val_loss: 0.1211 - val_accuracy: 0.9602\n",
      "Epoch 13/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0257 - accuracy: 0.9971 - val_loss: 0.1161 - val_accuracy: 0.9672\n",
      "Epoch 14/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0283 - accuracy: 0.9959 - val_loss: 0.1048 - val_accuracy: 0.9672\n",
      "Epoch 15/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0351 - accuracy: 0.9924 - val_loss: 0.1002 - val_accuracy: 0.9766\n",
      "Epoch 16/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0335 - accuracy: 0.9924 - val_loss: 0.1099 - val_accuracy: 0.9696\n",
      "Epoch 17/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0179 - accuracy: 0.9982 - val_loss: 0.1258 - val_accuracy: 0.9672\n",
      "Epoch 18/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0194 - accuracy: 0.9959 - val_loss: 0.1445 - val_accuracy: 0.9672\n",
      "Epoch 19/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0333 - accuracy: 0.9930 - val_loss: 0.1441 - val_accuracy: 0.9602\n",
      "Epoch 20/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0323 - accuracy: 0.9912 - val_loss: 0.1876 - val_accuracy: 0.9508\n",
      "Epoch 21/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0264 - accuracy: 0.9900 - val_loss: 0.1957 - val_accuracy: 0.9438\n",
      "Epoch 22/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0187 - accuracy: 0.9947 - val_loss: 0.1864 - val_accuracy: 0.9391\n",
      "Epoch 23/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0271 - accuracy: 0.9924 - val_loss: 0.1829 - val_accuracy: 0.9508\n",
      "Epoch 24/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0189 - accuracy: 0.9965 - val_loss: 0.1379 - val_accuracy: 0.9578\n",
      "Epoch 25/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0106 - accuracy: 0.9977 - val_loss: 0.1498 - val_accuracy: 0.9625\n",
      "Epoch 26/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0107 - accuracy: 0.9982 - val_loss: 0.1465 - val_accuracy: 0.9625\n",
      "Epoch 27/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0158 - accuracy: 0.9965 - val_loss: 0.2007 - val_accuracy: 0.9508\n",
      "Epoch 28/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0213 - accuracy: 0.9959 - val_loss: 0.1516 - val_accuracy: 0.9672\n",
      "Epoch 29/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0361 - accuracy: 0.9865 - val_loss: 0.1917 - val_accuracy: 0.9555\n",
      "Epoch 30/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0310 - accuracy: 0.9906 - val_loss: 0.1962 - val_accuracy: 0.9508\n",
      "Epoch 31/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0317 - accuracy: 0.9900 - val_loss: 0.1958 - val_accuracy: 0.9508\n",
      "Epoch 32/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0297 - accuracy: 0.9906 - val_loss: 0.2320 - val_accuracy: 0.9485\n",
      "Epoch 33/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0204 - accuracy: 0.9947 - val_loss: 0.1791 - val_accuracy: 0.9602\n",
      "Epoch 34/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0113 - accuracy: 0.9982 - val_loss: 0.1370 - val_accuracy: 0.9696\n",
      "Epoch 35/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0119 - accuracy: 0.9982 - val_loss: 0.1698 - val_accuracy: 0.9625\n",
      "Epoch 36/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0063 - accuracy: 0.9994 - val_loss: 0.1657 - val_accuracy: 0.9625\n",
      "Epoch 37/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0072 - accuracy: 0.9988 - val_loss: 0.1597 - val_accuracy: 0.9672\n",
      "Epoch 38/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0102 - accuracy: 0.9971 - val_loss: 0.1843 - val_accuracy: 0.9625\n",
      "Epoch 39/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0088 - accuracy: 0.9971 - val_loss: 0.1912 - val_accuracy: 0.9625\n",
      "Epoch 40/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0117 - accuracy: 0.9988 - val_loss: 0.1722 - val_accuracy: 0.9672\n",
      "Epoch 41/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.0053 - accuracy: 0.9988 - val_loss: 0.1683 - val_accuracy: 0.9672\n",
      "Epoch 42/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0100 - accuracy: 0.9977 - val_loss: 0.2142 - val_accuracy: 0.9555\n",
      "Epoch 43/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0086 - accuracy: 0.9977 - val_loss: 0.1550 - val_accuracy: 0.9696\n",
      "Epoch 44/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0155 - accuracy: 0.9959 - val_loss: 0.1323 - val_accuracy: 0.9696\n",
      "Epoch 45/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0093 - accuracy: 0.9988 - val_loss: 0.1806 - val_accuracy: 0.9602\n",
      "Epoch 46/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0116 - accuracy: 0.9977 - val_loss: 0.1312 - val_accuracy: 0.9672\n",
      "Epoch 47/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0220 - accuracy: 0.9947 - val_loss: 0.1379 - val_accuracy: 0.9696\n",
      "Epoch 48/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0160 - accuracy: 0.9953 - val_loss: 0.1523 - val_accuracy: 0.9625\n",
      "Epoch 49/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0193 - accuracy: 0.9941 - val_loss: 0.1609 - val_accuracy: 0.9555\n",
      "Epoch 50/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0131 - accuracy: 0.9959 - val_loss: 0.1207 - val_accuracy: 0.9696\n",
      "Epoch 51/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0124 - accuracy: 0.9977 - val_loss: 0.1308 - val_accuracy: 0.9719\n",
      "Epoch 52/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0057 - accuracy: 0.9982 - val_loss: 0.1488 - val_accuracy: 0.9649\n",
      "Epoch 53/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0083 - accuracy: 0.9977 - val_loss: 0.1289 - val_accuracy: 0.9719\n",
      "Epoch 54/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0050 - accuracy: 0.9988 - val_loss: 0.1147 - val_accuracy: 0.9742\n",
      "Epoch 55/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0060 - accuracy: 0.9982 - val_loss: 0.1230 - val_accuracy: 0.9766\n",
      "Epoch 56/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0032 - accuracy: 0.9994 - val_loss: 0.1576 - val_accuracy: 0.9696\n",
      "Epoch 57/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0036 - accuracy: 0.9994 - val_loss: 0.1746 - val_accuracy: 0.9672\n",
      "Epoch 58/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.1638 - val_accuracy: 0.9696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.1615 - val_accuracy: 0.9696\n",
      "Epoch 60/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.1892 - val_accuracy: 0.9672\n",
      "Epoch 61/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0036 - accuracy: 0.9988 - val_loss: 0.1694 - val_accuracy: 0.9672\n",
      "Epoch 62/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0051 - accuracy: 0.9982 - val_loss: 0.1854 - val_accuracy: 0.9625\n",
      "Epoch 63/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.2100 - val_accuracy: 0.9625\n",
      "Epoch 64/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0123 - accuracy: 0.9947 - val_loss: 0.2667 - val_accuracy: 0.9508\n",
      "Epoch 65/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0351 - accuracy: 0.9894 - val_loss: 0.1395 - val_accuracy: 0.9742\n",
      "Epoch 66/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0098 - accuracy: 0.9959 - val_loss: 0.1533 - val_accuracy: 0.9719\n",
      "Epoch 67/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0079 - accuracy: 0.9977 - val_loss: 0.1305 - val_accuracy: 0.9742\n",
      "Epoch 68/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0182 - accuracy: 0.9965 - val_loss: 0.1342 - val_accuracy: 0.9742\n",
      "Epoch 69/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0104 - accuracy: 0.9959 - val_loss: 0.1098 - val_accuracy: 0.9766\n",
      "Epoch 70/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0147 - accuracy: 0.9941 - val_loss: 0.1764 - val_accuracy: 0.9649\n",
      "Epoch 71/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0149 - accuracy: 0.9941 - val_loss: 0.1142 - val_accuracy: 0.9719\n",
      "Epoch 72/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0039 - accuracy: 0.9994 - val_loss: 0.2273 - val_accuracy: 0.9578\n",
      "Epoch 73/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0083 - accuracy: 0.9977 - val_loss: 0.1283 - val_accuracy: 0.9719\n",
      "Epoch 74/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0065 - accuracy: 0.9977 - val_loss: 0.3707 - val_accuracy: 0.9344\n",
      "Epoch 75/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0108 - accuracy: 0.9977 - val_loss: 0.1412 - val_accuracy: 0.9742\n",
      "Epoch 76/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0160 - accuracy: 0.9947 - val_loss: 0.1079 - val_accuracy: 0.9696\n",
      "Epoch 77/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0047 - accuracy: 0.9982 - val_loss: 0.1326 - val_accuracy: 0.9672\n",
      "Epoch 78/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0093 - accuracy: 0.9965 - val_loss: 0.1260 - val_accuracy: 0.9742\n",
      "Epoch 79/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0040 - accuracy: 0.9988 - val_loss: 0.2096 - val_accuracy: 0.9578\n",
      "Epoch 80/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0076 - accuracy: 0.9959 - val_loss: 0.1610 - val_accuracy: 0.9672\n",
      "Epoch 81/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0098 - accuracy: 0.9971 - val_loss: 0.2094 - val_accuracy: 0.9602\n",
      "Epoch 82/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0038 - accuracy: 0.9988 - val_loss: 0.1102 - val_accuracy: 0.9766\n",
      "Epoch 83/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0053 - accuracy: 0.9977 - val_loss: 0.1393 - val_accuracy: 0.9696\n",
      "Epoch 84/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0070 - accuracy: 0.9982 - val_loss: 0.1420 - val_accuracy: 0.9719\n",
      "Epoch 85/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0338 - accuracy: 0.9894 - val_loss: 0.1648 - val_accuracy: 0.9625\n",
      "Epoch 86/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0176 - accuracy: 0.9941 - val_loss: 0.1626 - val_accuracy: 0.9696\n",
      "Epoch 87/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0106 - accuracy: 0.9965 - val_loss: 0.1120 - val_accuracy: 0.9766\n",
      "Epoch 88/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0057 - accuracy: 0.9994 - val_loss: 0.1229 - val_accuracy: 0.9742\n",
      "Epoch 89/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0019 - accuracy: 0.9994 - val_loss: 0.1159 - val_accuracy: 0.9766\n",
      "Epoch 90/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0056 - accuracy: 0.9971 - val_loss: 0.1268 - val_accuracy: 0.9742\n",
      "Epoch 91/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0063 - accuracy: 0.9971 - val_loss: 0.1263 - val_accuracy: 0.9766\n",
      "Epoch 92/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0126 - accuracy: 0.9959 - val_loss: 0.1224 - val_accuracy: 0.9742\n",
      "Epoch 93/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0065 - accuracy: 0.9982 - val_loss: 0.1547 - val_accuracy: 0.9672\n",
      "Epoch 94/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0146 - accuracy: 0.9941 - val_loss: 0.0977 - val_accuracy: 0.9766\n",
      "Epoch 95/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0100 - accuracy: 0.9959 - val_loss: 0.1520 - val_accuracy: 0.9696\n",
      "Epoch 96/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0081 - accuracy: 0.9977 - val_loss: 0.1603 - val_accuracy: 0.9719\n",
      "Epoch 97/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0022 - accuracy: 0.9988 - val_loss: 0.1162 - val_accuracy: 0.9789\n",
      "Epoch 98/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0068 - accuracy: 0.9988 - val_loss: 0.1160 - val_accuracy: 0.9789\n",
      "Epoch 99/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.1194 - val_accuracy: 0.9789\n",
      "Epoch 100/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0035 - accuracy: 0.9988 - val_loss: 0.1325 - val_accuracy: 0.9742\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1325 - accuracy: 0.9742\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs = 100, validation_data= (x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7df8ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f6cfc453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting things ready to print classification report\n",
    "pred = model.predict(x_test)\n",
    "predict_classes = np.argmax(pred,axis=1)\n",
    "expected_classes = np.argmax(y_test,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6056bc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected:  (427,)\n",
      "Predicted:  (427,)\n",
      "Training Accuracy: 0.9742388758782201\n",
      "Classification Report:\n",
      "----------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       153\n",
      "           1       0.99      0.94      0.96       142\n",
      "           2       0.96      1.00      0.98       132\n",
      "\n",
      "    accuracy                           0.97       427\n",
      "   macro avg       0.97      0.97      0.97       427\n",
      "weighted avg       0.97      0.97      0.97       427\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Expected: \",expected_classes.shape)\n",
    "print(\"Predicted: \",predict_classes.shape)\n",
    "correct = accuracy_score(expected_classes,predict_classes)\n",
    "print(f\"Training Accuracy: {correct}\")\n",
    "cm = confusion_matrix(expected_classes, predict_classes)\n",
    "\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "sns.heatmap(cm, annot=True, vmin=0, fmt='g', cbar=False, cmap='Blues')\n",
    "\n",
    "\n",
    "clr = classification_report(expected_classes, predict_classes)\n",
    "print(\"Classification Report:\\n----------------------\\n\", clr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e2e632",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
