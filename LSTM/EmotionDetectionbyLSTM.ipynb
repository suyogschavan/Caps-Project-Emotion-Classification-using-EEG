{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fcfbbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e7171af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># mean_0_a</th>\n",
       "      <th>mean_1_a</th>\n",
       "      <th>mean_2_a</th>\n",
       "      <th>mean_3_a</th>\n",
       "      <th>mean_4_a</th>\n",
       "      <th>mean_d_0_a</th>\n",
       "      <th>mean_d_1_a</th>\n",
       "      <th>mean_d_2_a</th>\n",
       "      <th>mean_d_3_a</th>\n",
       "      <th>mean_d_4_a</th>\n",
       "      <th>...</th>\n",
       "      <th>fft_741_b</th>\n",
       "      <th>fft_742_b</th>\n",
       "      <th>fft_743_b</th>\n",
       "      <th>fft_744_b</th>\n",
       "      <th>fft_745_b</th>\n",
       "      <th>fft_746_b</th>\n",
       "      <th>fft_747_b</th>\n",
       "      <th>fft_748_b</th>\n",
       "      <th>fft_749_b</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.62</td>\n",
       "      <td>30.3</td>\n",
       "      <td>-356.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>26.3</td>\n",
       "      <td>1.070</td>\n",
       "      <td>0.411</td>\n",
       "      <td>-15.70</td>\n",
       "      <td>2.06</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>23.5</td>\n",
       "      <td>20.3</td>\n",
       "      <td>20.3</td>\n",
       "      <td>23.5</td>\n",
       "      <td>-215.0</td>\n",
       "      <td>280.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>280.00</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.80</td>\n",
       "      <td>33.1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>25.8</td>\n",
       "      <td>22.8</td>\n",
       "      <td>6.550</td>\n",
       "      <td>1.680</td>\n",
       "      <td>2.88</td>\n",
       "      <td>3.83</td>\n",
       "      <td>-4.82</td>\n",
       "      <td>...</td>\n",
       "      <td>-23.3</td>\n",
       "      <td>-21.8</td>\n",
       "      <td>-21.8</td>\n",
       "      <td>-23.3</td>\n",
       "      <td>182.0</td>\n",
       "      <td>2.57</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>2.57</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.90</td>\n",
       "      <td>29.4</td>\n",
       "      <td>-416.0</td>\n",
       "      <td>16.7</td>\n",
       "      <td>23.7</td>\n",
       "      <td>79.900</td>\n",
       "      <td>3.360</td>\n",
       "      <td>90.20</td>\n",
       "      <td>89.90</td>\n",
       "      <td>2.03</td>\n",
       "      <td>...</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-267.0</td>\n",
       "      <td>281.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>281.00</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.90</td>\n",
       "      <td>31.6</td>\n",
       "      <td>-143.0</td>\n",
       "      <td>19.8</td>\n",
       "      <td>24.3</td>\n",
       "      <td>-0.584</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>8.82</td>\n",
       "      <td>2.30</td>\n",
       "      <td>-1.97</td>\n",
       "      <td>...</td>\n",
       "      <td>299.0</td>\n",
       "      <td>-243.0</td>\n",
       "      <td>-243.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>9.53</td>\n",
       "      <td>9.53</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.30</td>\n",
       "      <td>31.3</td>\n",
       "      <td>45.2</td>\n",
       "      <td>27.3</td>\n",
       "      <td>24.5</td>\n",
       "      <td>34.800</td>\n",
       "      <td>-5.790</td>\n",
       "      <td>3.06</td>\n",
       "      <td>41.40</td>\n",
       "      <td>5.52</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>38.1</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.90</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2549 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   # mean_0_a  mean_1_a  mean_2_a  mean_3_a  mean_4_a  mean_d_0_a  mean_d_1_a  \\\n",
       "0        4.62      30.3    -356.0      15.6      26.3       1.070       0.411   \n",
       "1       28.80      33.1      32.0      25.8      22.8       6.550       1.680   \n",
       "2        8.90      29.4    -416.0      16.7      23.7      79.900       3.360   \n",
       "3       14.90      31.6    -143.0      19.8      24.3      -0.584      -0.284   \n",
       "4       28.30      31.3      45.2      27.3      24.5      34.800      -5.790   \n",
       "\n",
       "   mean_d_2_a  mean_d_3_a  mean_d_4_a  ...  fft_741_b  fft_742_b  fft_743_b  \\\n",
       "0      -15.70        2.06        3.15  ...       23.5       20.3       20.3   \n",
       "1        2.88        3.83       -4.82  ...      -23.3      -21.8      -21.8   \n",
       "2       90.20       89.90        2.03  ...      462.0     -233.0     -233.0   \n",
       "3        8.82        2.30       -1.97  ...      299.0     -243.0     -243.0   \n",
       "4        3.06       41.40        5.52  ...       12.0       38.1       38.1   \n",
       "\n",
       "   fft_744_b  fft_745_b  fft_746_b  fft_747_b  fft_748_b  fft_749_b     label  \n",
       "0       23.5     -215.0     280.00    -162.00    -162.00     280.00  NEGATIVE  \n",
       "1      -23.3      182.0       2.57     -31.60     -31.60       2.57   NEUTRAL  \n",
       "2      462.0     -267.0     281.00    -148.00    -148.00     281.00  POSITIVE  \n",
       "3      299.0      132.0     -12.40       9.53       9.53     -12.40  POSITIVE  \n",
       "4       12.0      119.0     -17.60      23.90      23.90     -17.60   NEUTRAL  \n",
       "\n",
       "[5 rows x 2549 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('H:/emotions.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca91a4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2132 entries, 0 to 2131\n",
      "Columns: 2549 entries, # mean_0_a to label\n",
      "dtypes: float64(2548), object(1)\n",
      "memory usage: 41.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6460325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='label', ylabel='count'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(x='label', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02dabb98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# mean_0_a    0\n",
       "mean_1_a      0\n",
       "mean_2_a      0\n",
       "mean_3_a      0\n",
       "mean_4_a      0\n",
       "             ..\n",
       "fft_746_b     0\n",
       "fft_747_b     0\n",
       "fft_748_b     0\n",
       "fft_749_b     0\n",
       "label         0\n",
       "Length: 2549, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()\n",
    "# no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7618375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   # mean_0_a  mean_1_a  mean_2_a  mean_3_a  mean_4_a  mean_d_0_a  mean_d_1_a  \\\n",
      "0        4.62      30.3    -356.0      15.6      26.3       1.070       0.411   \n",
      "1       28.80      33.1      32.0      25.8      22.8       6.550       1.680   \n",
      "2        8.90      29.4    -416.0      16.7      23.7      79.900       3.360   \n",
      "3       14.90      31.6    -143.0      19.8      24.3      -0.584      -0.284   \n",
      "4       28.30      31.3      45.2      27.3      24.5      34.800      -5.790   \n",
      "\n",
      "   mean_d_2_a  mean_d_3_a  mean_d_4_a  ...  fft_741_b  fft_742_b  fft_743_b  \\\n",
      "0      -15.70        2.06        3.15  ...       23.5       20.3       20.3   \n",
      "1        2.88        3.83       -4.82  ...      -23.3      -21.8      -21.8   \n",
      "2       90.20       89.90        2.03  ...      462.0     -233.0     -233.0   \n",
      "3        8.82        2.30       -1.97  ...      299.0     -243.0     -243.0   \n",
      "4        3.06       41.40        5.52  ...       12.0       38.1       38.1   \n",
      "\n",
      "   fft_744_b  fft_745_b  fft_746_b  fft_747_b  fft_748_b  fft_749_b  label  \n",
      "0       23.5     -215.0     280.00    -162.00    -162.00     280.00      2  \n",
      "1      -23.3      182.0       2.57     -31.60     -31.60       2.57      0  \n",
      "2      462.0     -267.0     281.00    -148.00    -148.00     281.00      1  \n",
      "3      299.0      132.0     -12.40       9.53       9.53     -12.40      1  \n",
      "4       12.0      119.0     -17.60      23.90      23.90     -17.60      0  \n",
      "\n",
      "[5 rows x 2549 columns]\n",
      "0    716\n",
      "2    708\n",
      "1    708\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "encode = ({'NEUTRAL': 0, 'POSITIVE': 1, 'NEGATIVE': 2} )\n",
    "#new dataset with replaced values\n",
    "df_encoded = df.replace(encode)\n",
    "\n",
    "print(df_encoded.head())\n",
    "print(df_encoded['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd8cad1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_encoded['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3b1c9e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># mean_0_a</th>\n",
       "      <th>mean_1_a</th>\n",
       "      <th>mean_2_a</th>\n",
       "      <th>mean_3_a</th>\n",
       "      <th>mean_4_a</th>\n",
       "      <th>mean_d_0_a</th>\n",
       "      <th>mean_d_1_a</th>\n",
       "      <th>mean_d_2_a</th>\n",
       "      <th>mean_d_3_a</th>\n",
       "      <th>mean_d_4_a</th>\n",
       "      <th>...</th>\n",
       "      <th>fft_741_b</th>\n",
       "      <th>fft_742_b</th>\n",
       "      <th>fft_743_b</th>\n",
       "      <th>fft_744_b</th>\n",
       "      <th>fft_745_b</th>\n",
       "      <th>fft_746_b</th>\n",
       "      <th>fft_747_b</th>\n",
       "      <th>fft_748_b</th>\n",
       "      <th>fft_749_b</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.62</td>\n",
       "      <td>30.3</td>\n",
       "      <td>-356.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>26.3</td>\n",
       "      <td>1.070</td>\n",
       "      <td>0.411</td>\n",
       "      <td>-15.70</td>\n",
       "      <td>2.06</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>23.5</td>\n",
       "      <td>20.3</td>\n",
       "      <td>20.3</td>\n",
       "      <td>23.5</td>\n",
       "      <td>-215.0</td>\n",
       "      <td>280.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>280.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.80</td>\n",
       "      <td>33.1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>25.8</td>\n",
       "      <td>22.8</td>\n",
       "      <td>6.550</td>\n",
       "      <td>1.680</td>\n",
       "      <td>2.88</td>\n",
       "      <td>3.83</td>\n",
       "      <td>-4.82</td>\n",
       "      <td>...</td>\n",
       "      <td>-23.3</td>\n",
       "      <td>-21.8</td>\n",
       "      <td>-21.8</td>\n",
       "      <td>-23.3</td>\n",
       "      <td>182.0</td>\n",
       "      <td>2.57</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>2.57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.90</td>\n",
       "      <td>29.4</td>\n",
       "      <td>-416.0</td>\n",
       "      <td>16.7</td>\n",
       "      <td>23.7</td>\n",
       "      <td>79.900</td>\n",
       "      <td>3.360</td>\n",
       "      <td>90.20</td>\n",
       "      <td>89.90</td>\n",
       "      <td>2.03</td>\n",
       "      <td>...</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-267.0</td>\n",
       "      <td>281.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>281.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.90</td>\n",
       "      <td>31.6</td>\n",
       "      <td>-143.0</td>\n",
       "      <td>19.8</td>\n",
       "      <td>24.3</td>\n",
       "      <td>-0.584</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>8.82</td>\n",
       "      <td>2.30</td>\n",
       "      <td>-1.97</td>\n",
       "      <td>...</td>\n",
       "      <td>299.0</td>\n",
       "      <td>-243.0</td>\n",
       "      <td>-243.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>9.53</td>\n",
       "      <td>9.53</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.30</td>\n",
       "      <td>31.3</td>\n",
       "      <td>45.2</td>\n",
       "      <td>27.3</td>\n",
       "      <td>24.5</td>\n",
       "      <td>34.800</td>\n",
       "      <td>-5.790</td>\n",
       "      <td>3.06</td>\n",
       "      <td>41.40</td>\n",
       "      <td>5.52</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>38.1</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.90</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2549 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   # mean_0_a  mean_1_a  mean_2_a  mean_3_a  mean_4_a  mean_d_0_a  mean_d_1_a  \\\n",
       "0        4.62      30.3    -356.0      15.6      26.3       1.070       0.411   \n",
       "1       28.80      33.1      32.0      25.8      22.8       6.550       1.680   \n",
       "2        8.90      29.4    -416.0      16.7      23.7      79.900       3.360   \n",
       "3       14.90      31.6    -143.0      19.8      24.3      -0.584      -0.284   \n",
       "4       28.30      31.3      45.2      27.3      24.5      34.800      -5.790   \n",
       "\n",
       "   mean_d_2_a  mean_d_3_a  mean_d_4_a  ...  fft_741_b  fft_742_b  fft_743_b  \\\n",
       "0      -15.70        2.06        3.15  ...       23.5       20.3       20.3   \n",
       "1        2.88        3.83       -4.82  ...      -23.3      -21.8      -21.8   \n",
       "2       90.20       89.90        2.03  ...      462.0     -233.0     -233.0   \n",
       "3        8.82        2.30       -1.97  ...      299.0     -243.0     -243.0   \n",
       "4        3.06       41.40        5.52  ...       12.0       38.1       38.1   \n",
       "\n",
       "   fft_744_b  fft_745_b  fft_746_b  fft_747_b  fft_748_b  fft_749_b  label  \n",
       "0       23.5     -215.0     280.00    -162.00    -162.00     280.00      2  \n",
       "1      -23.3      182.0       2.57     -31.60     -31.60       2.57      0  \n",
       "2      462.0     -267.0     281.00    -148.00    -148.00     281.00      1  \n",
       "3      299.0      132.0     -12.40       9.53       9.53     -12.40      1  \n",
       "4       12.0      119.0     -17.60      23.90      23.90     -17.60      0  \n",
       "\n",
       "[5 rows x 2549 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc51b6fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2132, 2548)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=df_encoded.drop([\"label\"]  ,axis=1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00d4d4c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2132,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df_encoded.loc[:,'label'].values\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce734b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x)\n",
    "x = scaler.transform(x)\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y = to_categorical(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b5e64aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e52f282",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "063e2c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.reshape(x_train, (x_train.shape[0],1,x.shape[1]))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0],1,x.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76a3eaed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 1, 64)             668928    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1, 64)             0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 32)                12416     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 681,443\n",
      "Trainable params: 681,443\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(1,2548),activation=\"relu\",return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(32,activation=\"sigmoid\"))\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(LSTM(100,return_sequences=True))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(LSTM(50))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = \"adam\", metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f91a52f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "54/54 [==============================] - 2s 12ms/step - loss: 0.5956 - accuracy: 0.7977 - val_loss: 0.3654 - val_accuracy: 0.9180\n",
      "Epoch 2/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.3100 - accuracy: 0.9337 - val_loss: 0.2803 - val_accuracy: 0.9180\n",
      "Epoch 3/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.2199 - accuracy: 0.9431 - val_loss: 0.2191 - val_accuracy: 0.9344\n",
      "Epoch 4/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.1894 - accuracy: 0.9443 - val_loss: 0.1975 - val_accuracy: 0.9321\n",
      "Epoch 5/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.1487 - accuracy: 0.9630 - val_loss: 0.1980 - val_accuracy: 0.9227\n",
      "Epoch 6/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.1246 - accuracy: 0.9683 - val_loss: 0.1661 - val_accuracy: 0.9368\n",
      "Epoch 7/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.1064 - accuracy: 0.9730 - val_loss: 0.1513 - val_accuracy: 0.9391\n",
      "Epoch 8/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0749 - accuracy: 0.9848 - val_loss: 0.1171 - val_accuracy: 0.9672\n",
      "Epoch 9/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0545 - accuracy: 0.9889 - val_loss: 0.1206 - val_accuracy: 0.9672\n",
      "Epoch 10/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0454 - accuracy: 0.9935 - val_loss: 0.1480 - val_accuracy: 0.9602\n",
      "Epoch 11/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0403 - accuracy: 0.9941 - val_loss: 0.1191 - val_accuracy: 0.9696\n",
      "Epoch 12/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0365 - accuracy: 0.9947 - val_loss: 0.1211 - val_accuracy: 0.9602\n",
      "Epoch 13/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0257 - accuracy: 0.9971 - val_loss: 0.1161 - val_accuracy: 0.9672\n",
      "Epoch 14/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0283 - accuracy: 0.9959 - val_loss: 0.1048 - val_accuracy: 0.9672\n",
      "Epoch 15/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0351 - accuracy: 0.9924 - val_loss: 0.1002 - val_accuracy: 0.9766\n",
      "Epoch 16/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0335 - accuracy: 0.9924 - val_loss: 0.1099 - val_accuracy: 0.9696\n",
      "Epoch 17/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0179 - accuracy: 0.9982 - val_loss: 0.1258 - val_accuracy: 0.9672\n",
      "Epoch 18/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0194 - accuracy: 0.9959 - val_loss: 0.1445 - val_accuracy: 0.9672\n",
      "Epoch 19/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0333 - accuracy: 0.9930 - val_loss: 0.1441 - val_accuracy: 0.9602\n",
      "Epoch 20/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0323 - accuracy: 0.9912 - val_loss: 0.1876 - val_accuracy: 0.9508\n",
      "Epoch 21/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0264 - accuracy: 0.9900 - val_loss: 0.1957 - val_accuracy: 0.9438\n",
      "Epoch 22/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0187 - accuracy: 0.9947 - val_loss: 0.1864 - val_accuracy: 0.9391\n",
      "Epoch 23/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0271 - accuracy: 0.9924 - val_loss: 0.1829 - val_accuracy: 0.9508\n",
      "Epoch 24/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0189 - accuracy: 0.9965 - val_loss: 0.1379 - val_accuracy: 0.9578\n",
      "Epoch 25/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0106 - accuracy: 0.9977 - val_loss: 0.1498 - val_accuracy: 0.9625\n",
      "Epoch 26/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0107 - accuracy: 0.9982 - val_loss: 0.1465 - val_accuracy: 0.9625\n",
      "Epoch 27/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0158 - accuracy: 0.9965 - val_loss: 0.2007 - val_accuracy: 0.9508\n",
      "Epoch 28/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0213 - accuracy: 0.9959 - val_loss: 0.1516 - val_accuracy: 0.9672\n",
      "Epoch 29/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0361 - accuracy: 0.9865 - val_loss: 0.1917 - val_accuracy: 0.9555\n",
      "Epoch 30/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0310 - accuracy: 0.9906 - val_loss: 0.1962 - val_accuracy: 0.9508\n",
      "Epoch 31/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0317 - accuracy: 0.9900 - val_loss: 0.1958 - val_accuracy: 0.9508\n",
      "Epoch 32/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0297 - accuracy: 0.9906 - val_loss: 0.2320 - val_accuracy: 0.9485\n",
      "Epoch 33/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0204 - accuracy: 0.9947 - val_loss: 0.1791 - val_accuracy: 0.9602\n",
      "Epoch 34/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0113 - accuracy: 0.9982 - val_loss: 0.1370 - val_accuracy: 0.9696\n",
      "Epoch 35/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0119 - accuracy: 0.9982 - val_loss: 0.1698 - val_accuracy: 0.9625\n",
      "Epoch 36/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0063 - accuracy: 0.9994 - val_loss: 0.1657 - val_accuracy: 0.9625\n",
      "Epoch 37/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0072 - accuracy: 0.9988 - val_loss: 0.1597 - val_accuracy: 0.9672\n",
      "Epoch 38/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0102 - accuracy: 0.9971 - val_loss: 0.1843 - val_accuracy: 0.9625\n",
      "Epoch 39/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0088 - accuracy: 0.9971 - val_loss: 0.1912 - val_accuracy: 0.9625\n",
      "Epoch 40/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0117 - accuracy: 0.9988 - val_loss: 0.1722 - val_accuracy: 0.9672\n",
      "Epoch 41/100\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.0053 - accuracy: 0.9988 - val_loss: 0.1683 - val_accuracy: 0.9672\n",
      "Epoch 42/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0100 - accuracy: 0.9977 - val_loss: 0.2142 - val_accuracy: 0.9555\n",
      "Epoch 43/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0086 - accuracy: 0.9977 - val_loss: 0.1550 - val_accuracy: 0.9696\n",
      "Epoch 44/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0155 - accuracy: 0.9959 - val_loss: 0.1323 - val_accuracy: 0.9696\n",
      "Epoch 45/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0093 - accuracy: 0.9988 - val_loss: 0.1806 - val_accuracy: 0.9602\n",
      "Epoch 46/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0116 - accuracy: 0.9977 - val_loss: 0.1312 - val_accuracy: 0.9672\n",
      "Epoch 47/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0220 - accuracy: 0.9947 - val_loss: 0.1379 - val_accuracy: 0.9696\n",
      "Epoch 48/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0160 - accuracy: 0.9953 - val_loss: 0.1523 - val_accuracy: 0.9625\n",
      "Epoch 49/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0193 - accuracy: 0.9941 - val_loss: 0.1609 - val_accuracy: 0.9555\n",
      "Epoch 50/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0131 - accuracy: 0.9959 - val_loss: 0.1207 - val_accuracy: 0.9696\n",
      "Epoch 51/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0124 - accuracy: 0.9977 - val_loss: 0.1308 - val_accuracy: 0.9719\n",
      "Epoch 52/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0057 - accuracy: 0.9982 - val_loss: 0.1488 - val_accuracy: 0.9649\n",
      "Epoch 53/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0083 - accuracy: 0.9977 - val_loss: 0.1289 - val_accuracy: 0.9719\n",
      "Epoch 54/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0050 - accuracy: 0.9988 - val_loss: 0.1147 - val_accuracy: 0.9742\n",
      "Epoch 55/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0060 - accuracy: 0.9982 - val_loss: 0.1230 - val_accuracy: 0.9766\n",
      "Epoch 56/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0032 - accuracy: 0.9994 - val_loss: 0.1576 - val_accuracy: 0.9696\n",
      "Epoch 57/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0036 - accuracy: 0.9994 - val_loss: 0.1746 - val_accuracy: 0.9672\n",
      "Epoch 58/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.1638 - val_accuracy: 0.9696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.1615 - val_accuracy: 0.9696\n",
      "Epoch 60/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.1892 - val_accuracy: 0.9672\n",
      "Epoch 61/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0036 - accuracy: 0.9988 - val_loss: 0.1694 - val_accuracy: 0.9672\n",
      "Epoch 62/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0051 - accuracy: 0.9982 - val_loss: 0.1854 - val_accuracy: 0.9625\n",
      "Epoch 63/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.2100 - val_accuracy: 0.9625\n",
      "Epoch 64/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0123 - accuracy: 0.9947 - val_loss: 0.2667 - val_accuracy: 0.9508\n",
      "Epoch 65/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0351 - accuracy: 0.9894 - val_loss: 0.1395 - val_accuracy: 0.9742\n",
      "Epoch 66/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0098 - accuracy: 0.9959 - val_loss: 0.1533 - val_accuracy: 0.9719\n",
      "Epoch 67/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0079 - accuracy: 0.9977 - val_loss: 0.1305 - val_accuracy: 0.9742\n",
      "Epoch 68/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0182 - accuracy: 0.9965 - val_loss: 0.1342 - val_accuracy: 0.9742\n",
      "Epoch 69/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0104 - accuracy: 0.9959 - val_loss: 0.1098 - val_accuracy: 0.9766\n",
      "Epoch 70/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0147 - accuracy: 0.9941 - val_loss: 0.1764 - val_accuracy: 0.9649\n",
      "Epoch 71/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0149 - accuracy: 0.9941 - val_loss: 0.1142 - val_accuracy: 0.9719\n",
      "Epoch 72/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0039 - accuracy: 0.9994 - val_loss: 0.2273 - val_accuracy: 0.9578\n",
      "Epoch 73/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0083 - accuracy: 0.9977 - val_loss: 0.1283 - val_accuracy: 0.9719\n",
      "Epoch 74/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0065 - accuracy: 0.9977 - val_loss: 0.3707 - val_accuracy: 0.9344\n",
      "Epoch 75/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0108 - accuracy: 0.9977 - val_loss: 0.1412 - val_accuracy: 0.9742\n",
      "Epoch 76/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0160 - accuracy: 0.9947 - val_loss: 0.1079 - val_accuracy: 0.9696\n",
      "Epoch 77/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0047 - accuracy: 0.9982 - val_loss: 0.1326 - val_accuracy: 0.9672\n",
      "Epoch 78/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0093 - accuracy: 0.9965 - val_loss: 0.1260 - val_accuracy: 0.9742\n",
      "Epoch 79/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0040 - accuracy: 0.9988 - val_loss: 0.2096 - val_accuracy: 0.9578\n",
      "Epoch 80/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0076 - accuracy: 0.9959 - val_loss: 0.1610 - val_accuracy: 0.9672\n",
      "Epoch 81/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0098 - accuracy: 0.9971 - val_loss: 0.2094 - val_accuracy: 0.9602\n",
      "Epoch 82/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0038 - accuracy: 0.9988 - val_loss: 0.1102 - val_accuracy: 0.9766\n",
      "Epoch 83/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0053 - accuracy: 0.9977 - val_loss: 0.1393 - val_accuracy: 0.9696\n",
      "Epoch 84/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0070 - accuracy: 0.9982 - val_loss: 0.1420 - val_accuracy: 0.9719\n",
      "Epoch 85/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0338 - accuracy: 0.9894 - val_loss: 0.1648 - val_accuracy: 0.9625\n",
      "Epoch 86/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0176 - accuracy: 0.9941 - val_loss: 0.1626 - val_accuracy: 0.9696\n",
      "Epoch 87/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0106 - accuracy: 0.9965 - val_loss: 0.1120 - val_accuracy: 0.9766\n",
      "Epoch 88/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0057 - accuracy: 0.9994 - val_loss: 0.1229 - val_accuracy: 0.9742\n",
      "Epoch 89/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0019 - accuracy: 0.9994 - val_loss: 0.1159 - val_accuracy: 0.9766\n",
      "Epoch 90/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0056 - accuracy: 0.9971 - val_loss: 0.1268 - val_accuracy: 0.9742\n",
      "Epoch 91/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0063 - accuracy: 0.9971 - val_loss: 0.1263 - val_accuracy: 0.9766\n",
      "Epoch 92/100\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.0126 - accuracy: 0.9959 - val_loss: 0.1224 - val_accuracy: 0.9742\n",
      "Epoch 93/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0065 - accuracy: 0.9982 - val_loss: 0.1547 - val_accuracy: 0.9672\n",
      "Epoch 94/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0146 - accuracy: 0.9941 - val_loss: 0.0977 - val_accuracy: 0.9766\n",
      "Epoch 95/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0100 - accuracy: 0.9959 - val_loss: 0.1520 - val_accuracy: 0.9696\n",
      "Epoch 96/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0081 - accuracy: 0.9977 - val_loss: 0.1603 - val_accuracy: 0.9719\n",
      "Epoch 97/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0022 - accuracy: 0.9988 - val_loss: 0.1162 - val_accuracy: 0.9789\n",
      "Epoch 98/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0068 - accuracy: 0.9988 - val_loss: 0.1160 - val_accuracy: 0.9789\n",
      "Epoch 99/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.1194 - val_accuracy: 0.9789\n",
      "Epoch 100/100\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.0035 - accuracy: 0.9988 - val_loss: 0.1325 - val_accuracy: 0.9742\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1325 - accuracy: 0.9742\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs = 100, validation_data= (x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7df8ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f6cfc453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting things ready to print classification report\n",
    "pred = model.predict(x_test)\n",
    "predict_classes = np.argmax(pred,axis=1)\n",
    "expected_classes = np.argmax(y_test,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d8a5e9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected:  [1 1 1 2 1 1 2 1 2 1 2 2 2 1 2 1 2 0 0 0 0 1 0 0 0 1 1 1 0 0 2 0 0 0 0 0 0\n",
      " 1 1 2 1 0 2 2 0 2 1 1 0 2 0 1 1 1 2 0 2 0 2 2 1 0 1 1 2 2 2 0 1 2 0 0 0 2\n",
      " 1 1 0 1 1 2 1 0 1 1 1 2 1 1 0 2 1 1 0 2 0 1 0 0 0 1 2 1 2 0 0 2 0 2 0 1 0\n",
      " 1 1 2 1 0 1 1 2 2 0 2 1 0 2 2 0 1 0 2 2 0 1 2 2 2 0 0 2 1 2 1 2 0 1 1 1 0\n",
      " 0 2 0 2 2 2 2 1 1 2 2 2 2 0 0 1 2 1 2 1 2 1 1 1 0 2 1 1 0 1 0 2 0 1 0 1 0\n",
      " 2 0 0 0 0 1 2 1 1 0 2 1 2 2 1 0 0 2 2 0 0 0 0 2 1 1 0 2 0 0 0 2 1 1 0 1 0\n",
      " 0 1 0 1 0 2 0 2 0 1 2 1 2 2 0 0 2 1 1 1 1 0 0 0 0 2 0 1 2 2 2 1 1 1 1 2 1\n",
      " 0 0 0 0 1 0 1 1 1 2 0 0 1 2 2 1 2 1 0 0 1 2 0 2 2 1 0 2 0 0 2 2 2 0 0 0 0\n",
      " 0 1 2 2 1 0 2 1 0 0 1 1 2 0 0 1 0 0 2 1 0 0 2 2 0 2 1 0 0 1 1 1 2 2 2 0 2\n",
      " 0 2 2 2 1 2 2 1 2 2 0 1 2 0 0 2 1 0 0 1 2 1 1 0 0 0 2 2 1 0 1 0 1 2 2 0 1\n",
      " 0 1 0 1 2 1 0 2 0 0 1 2 2 0 1 0 0 1 1 2 2 1 0 1 0 0 1 0 0 1 1 0 0 1 2 2 0\n",
      " 1 2 2 0 0 0 1 1 0 2 0 2 1 2 2 2 1 1 0 1]\n",
      "Predicted:  [1 1 1 2 1 1 2 1 2 1 2 2 2 1 2 1 2 0 0 0 0 1 0 0 0 1 1 1 0 0 2 0 0 0 0 0 0\n",
      " 1 1 2 1 0 2 2 0 2 1 1 0 2 0 1 1 1 2 0 2 0 2 2 1 0 1 1 2 2 2 0 1 2 0 0 0 2\n",
      " 1 1 0 1 1 2 1 0 1 1 1 2 1 1 0 2 1 1 0 2 0 1 0 0 0 1 2 1 2 0 0 2 0 2 0 1 0\n",
      " 1 1 2 1 0 1 1 2 2 0 2 1 0 2 2 0 1 0 2 2 0 1 2 2 2 0 0 2 1 2 1 2 0 1 1 1 0\n",
      " 0 2 0 2 2 2 2 1 1 2 2 2 2 0 0 1 2 1 2 1 2 1 1 1 0 2 1 0 0 1 0 2 0 1 0 1 0\n",
      " 2 0 0 0 0 2 2 1 1 0 2 1 2 2 2 0 0 2 2 0 0 0 0 2 1 1 0 2 0 0 0 2 1 1 0 1 0\n",
      " 0 1 0 1 0 2 0 2 0 2 2 1 2 2 0 0 2 1 1 1 1 0 0 0 1 2 0 1 2 2 2 1 1 1 1 2 1\n",
      " 0 0 0 0 1 0 1 1 1 2 0 0 2 2 2 1 2 1 0 0 1 2 0 2 2 1 0 2 0 0 2 2 2 0 0 0 0\n",
      " 0 1 2 2 1 0 2 1 0 0 1 1 2 0 0 1 0 0 2 1 0 0 2 2 0 2 1 0 0 1 1 1 2 2 2 0 2\n",
      " 0 2 2 2 1 2 2 1 2 2 0 1 2 0 0 2 1 0 0 1 2 1 1 0 1 0 2 2 1 0 1 0 1 2 2 0 1\n",
      " 0 2 0 1 2 1 0 2 0 0 1 2 2 0 2 0 0 1 1 2 2 1 0 1 0 0 1 0 0 1 1 0 0 0 2 2 0\n",
      " 1 2 2 0 0 0 1 0 0 2 0 2 1 2 2 2 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Expected: \",expected_classes)\n",
    "print(\"Predicted: \",predict_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "062351ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9742388758782201\n",
      "Classification Report:\n",
      "----------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       153\n",
      "           1       0.99      0.94      0.96       142\n",
      "           2       0.96      1.00      0.98       132\n",
      "\n",
      "    accuracy                           0.97       427\n",
      "   macro avg       0.97      0.97      0.97       427\n",
      "weighted avg       0.97      0.97      0.97       427\n",
      "\n"
     ]
    }
   ],
   "source": [
    "correct = accuracy_score(expected_classes,predict_classes)\n",
    "print(f\"Training Accuracy: {correct}\")\n",
    "cm = confusion_matrix(expected_classes, predict_classes)\n",
    "\n",
    "clr = classification_report(expected_classes, predict_classes)\n",
    "print(\"Classification Report:\\n----------------------\\n\", clr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "80528efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Confusion Matrix')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(cm, annot=True, vmin=0, fmt='g', cbar=False, cmap='Blues')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46f7f41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
